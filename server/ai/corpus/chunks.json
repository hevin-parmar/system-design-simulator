[
  {
    "id": "chunk-1",
    "docId": "fundamentals",
    "text": "# System Design Fundamentals\n\n## Load Balancer\n\n**Routing strategies**\n- **Round robin:** Rotate through backends. Simple, no session awareness.\n- **Least connections:** Send to backend with fewest active connections. Better for uneven request duration.\n- **IP hash:** Same client IP → same backend. Enables sticky routing without app support.\n\n**Health checks:** Periodic probes (HTTP/TCP) to detect dead or overloaded backends. Remove from pool on failure; re-add when healthy. Typical interval 5–30s.\n\n**L4 vs L7:** L4 operates on IP/port; L7 sees HTTP, supports path-based routing, header inspection, and TLS termination.\n\n---\n\n## Cache\n\n**Patterns**\n- **Read-through:** App reads from cache; cache loads from DB on miss.\n- **Write-through:** Write goes to cache and DB synchronously. Strong consistency, higher latency.\n- **Write-back (write-behind):** Write to cache; async flush to DB. Low latency, risk of data loss on crash.\n\n**TTL:** Expiry time for entries. Balances freshness vs hit rate. Short TTL = fresher, more DB load.\n\n**Invalidation:** On write, invalidate or update cached copies. Hard cache invalidation is tricky in distributed setups—often use TTL + versioned keys or event-driven invalidation.\n\n---\n\n## Sharding",
    "tokens": [
      "system",
      "design",
      "fundamentals",
      "load",
      "balancer",
      "routing",
      "strategies",
      "-",
      "round",
      "robin",
      "rotate",
      "through",
      "backends",
      "simple",
      "no",
      "session",
      "awareness",
      "-",
      "least",
      "connections",
      "send",
      "to",
      "backend",
      "with",
      "fewest",
      "active",
      "connections",
      "better",
      "for",
      "uneven",
      "request",
      "duration",
      "-",
      "ip",
      "hash",
      "same",
      "client",
      "ip",
      "same",
      "backend",
      "enables",
      "sticky",
      "routing",
      "without",
      "app",
      "support",
      "health",
      "checks",
      "periodic",
      "probes",
      "http",
      "tcp",
      "to",
      "detect",
      "dead",
      "or",
      "overloaded",
      "backends",
      "remove",
      "from",
      "pool",
      "on",
      "failure",
      "re-add",
      "when",
      "healthy",
      "typical",
      "interval",
      "5",
      "30s",
      "l4",
      "vs",
      "l7",
      "l4",
      "operates",
      "on",
      "ip",
      "port",
      "l7",
      "sees",
      "http",
      "supports",
      "path-based",
      "routing",
      "header",
      "inspection",
      "and",
      "tls",
      "termination",
      "---",
      "cache",
      "patterns",
      "-",
      "read-through",
      "app",
      "reads",
      "from",
      "cache",
      "cache",
      "loads",
      "from",
      "db",
      "on",
      "miss",
      "-",
      "write-through",
      "write",
      "goes",
      "to",
      "cache",
      "and",
      "db",
      "synchronously",
      "strong",
      "consistency",
      "higher",
      "latency",
      "-",
      "write-back",
      "write-behind",
      "write",
      "to",
      "cache",
      "async",
      "flush",
      "to",
      "db",
      "low",
      "latency",
      "risk",
      "of",
      "data",
      "loss",
      "on",
      "crash",
      "ttl",
      "expiry",
      "time",
      "for",
      "entries",
      "balances",
      "freshness",
      "vs",
      "hit",
      "rate",
      "short",
      "ttl",
      "fresher",
      "more",
      "db",
      "load",
      "invalidation",
      "on",
      "write",
      "invalidate",
      "or",
      "update",
      "cached",
      "copies",
      "hard",
      "cache",
      "invalidation",
      "is",
      "tricky",
      "in",
      "distributed",
      "setups",
      "often",
      "use",
      "ttl",
      "versioned",
      "keys",
      "or",
      "event-driven",
      "invalidation",
      "---",
      "sharding"
    ]
  },
  {
    "id": "chunk-2",
    "docId": "fundamentals",
    "text": "**Strategies**\n- **Hash-based:** `partition_key = hash(user_id) % num_shards`. Even distribution; range queries difficult.\n- **Range-based:** Partition by key range (e.g. A–M, N–Z). Good for range scans; risk of hot shards at boundaries.\n\n**Hot partitions:** One shard gets disproportionate load. Mitigate with secondary partition key, or split hot shards.\n\n---\n\n## Message Queues\n\n**Delivery semantics**\n- **At-least-once:** Consumer acks after processing. Retries on failure; duplicates possible.\n- **Exactly-once:** Requires idempotent consumers + transactional outbox or deduplication.\n\n**Retries:** Exponential backoff. Limit retries to avoid infinite loops.\n\n**DLQ (Dead Letter Queue):** Messages that exceed retries go to DLQ for inspection or replay.\n\n**Backpressure:** When consumers lag, producers slow down or reject. Prevents queue unbounded growth.\n\n---\n\n## Databases\n\n**Primary / Replica**\n- Primary handles writes; replicas replicate via WAL or binlog.\n- Replicas serve reads; reduce load on primary.\n- **Replication lag:** Writes on primary not yet visible on replica. Affects read-after-write consistency.\n\n---\n\n## Consistency & Availability\n\n**Eventual consistency:** Replicas converge over time. Acceptable when immediate consistency not required (e.g. counters, analytics).\n\n**Quorum:** For N replicas, write to W and read from R. If W + R > N, read sees latest write. Tradeoffs: higher W/R = stronger consistency, lower availability.",
    "tokens": [
      "strategies",
      "-",
      "hash-based",
      "partition_key",
      "hash",
      "user_id",
      "num_shards",
      "even",
      "distribution",
      "range",
      "queries",
      "difficult",
      "-",
      "range-based",
      "partition",
      "by",
      "key",
      "range",
      "e",
      "g",
      "a",
      "m",
      "n",
      "z",
      "good",
      "for",
      "range",
      "scans",
      "risk",
      "of",
      "hot",
      "shards",
      "at",
      "boundaries",
      "hot",
      "partitions",
      "one",
      "shard",
      "gets",
      "disproportionate",
      "load",
      "mitigate",
      "with",
      "secondary",
      "partition",
      "key",
      "or",
      "split",
      "hot",
      "shards",
      "---",
      "message",
      "queues",
      "delivery",
      "semantics",
      "-",
      "at-least-once",
      "consumer",
      "acks",
      "after",
      "processing",
      "retries",
      "on",
      "failure",
      "duplicates",
      "possible",
      "-",
      "exactly-once",
      "requires",
      "idempotent",
      "consumers",
      "transactional",
      "outbox",
      "or",
      "deduplication",
      "retries",
      "exponential",
      "backoff",
      "limit",
      "retries",
      "to",
      "avoid",
      "infinite",
      "loops",
      "dlq",
      "dead",
      "letter",
      "queue",
      "messages",
      "that",
      "exceed",
      "retries",
      "go",
      "to",
      "dlq",
      "for",
      "inspection",
      "or",
      "replay",
      "backpressure",
      "when",
      "consumers",
      "lag",
      "producers",
      "slow",
      "down",
      "or",
      "reject",
      "prevents",
      "queue",
      "unbounded",
      "growth",
      "---",
      "databases",
      "primary",
      "replica",
      "-",
      "primary",
      "handles",
      "writes",
      "replicas",
      "replicate",
      "via",
      "wal",
      "or",
      "binlog",
      "-",
      "replicas",
      "serve",
      "reads",
      "reduce",
      "load",
      "on",
      "primary",
      "-",
      "replication",
      "lag",
      "writes",
      "on",
      "primary",
      "not",
      "yet",
      "visible",
      "on",
      "replica",
      "affects",
      "read-after-write",
      "consistency",
      "---",
      "consistency",
      "availability",
      "eventual",
      "consistency",
      "replicas",
      "converge",
      "over",
      "time",
      "acceptable",
      "when",
      "immediate",
      "consistency",
      "not",
      "required",
      "e",
      "g",
      "counters",
      "analytics",
      "quorum",
      "for",
      "n",
      "replicas",
      "write",
      "to",
      "w",
      "and",
      "read",
      "from",
      "r",
      "if",
      "w",
      "r",
      "n",
      "read",
      "sees",
      "latest",
      "write",
      "tradeoffs",
      "higher",
      "w",
      "r",
      "stronger",
      "consistency",
      "lower",
      "availability"
    ]
  },
  {
    "id": "chunk-3",
    "docId": "fundamentals",
    "text": "**CAP:** Under partition, choose consistency or availability. In practice, tune per operation.\n\n---\n\n## SLO / SLI / Error Budgets\n\n**SLI:** Measured metric (e.g. latency p99, error rate).\n**SLO:** Target for SLI (e.g. p99 < 200ms, 99.9% success).\n**Error budget:** Allowed failure before SLO breached. Drives release and incident response decisions.\n\n---\n\n## Observability\n\n**Logs:** Event records. Structured (JSON) for aggregation.\n**Metrics:** Aggregated counters/gauges (QPS, latency percentiles, error rate).\n**Traces:** Request flow across services. Correlate with trace IDs.\n\n---\n\n## Cost Basics\n\n**Hot vs cold storage:** Hot = low-latency, higher cost (SSD, in-memory). Cold = archival, cheaper (object storage, infrequent access). Tier by access pattern.\n",
    "tokens": [
      "cap",
      "under",
      "partition",
      "choose",
      "consistency",
      "or",
      "availability",
      "in",
      "practice",
      "tune",
      "per",
      "operation",
      "---",
      "slo",
      "sli",
      "error",
      "budgets",
      "sli",
      "measured",
      "metric",
      "e",
      "g",
      "latency",
      "p99",
      "error",
      "rate",
      "slo",
      "target",
      "for",
      "sli",
      "e",
      "g",
      "p99",
      "200ms",
      "99",
      "9",
      "success",
      "error",
      "budget",
      "allowed",
      "failure",
      "before",
      "slo",
      "breached",
      "drives",
      "release",
      "and",
      "incident",
      "response",
      "decisions",
      "---",
      "observability",
      "logs",
      "event",
      "records",
      "structured",
      "json",
      "for",
      "aggregation",
      "metrics",
      "aggregated",
      "counters",
      "gauges",
      "qps",
      "latency",
      "percentiles",
      "error",
      "rate",
      "traces",
      "request",
      "flow",
      "across",
      "services",
      "correlate",
      "with",
      "trace",
      "ids",
      "---",
      "cost",
      "basics",
      "hot",
      "vs",
      "cold",
      "storage",
      "hot",
      "low-latency",
      "higher",
      "cost",
      "ssd",
      "in-memory",
      "cold",
      "archival",
      "cheaper",
      "object",
      "storage",
      "infrequent",
      "access",
      "tier",
      "by",
      "access",
      "pattern"
    ]
  },
  {
    "id": "chunk-4",
    "docId": "load_balancer",
    "text": "# Load Balancer\n\n## What it is\n\nA component that distributes incoming traffic across multiple backend servers. Sits between clients and application servers. Can operate at L4 (TCP/UDP) or L7 (HTTP/HTTPS).\n\n## When to use it\n\n- Multiple app server instances for horizontal scaling\n- Need high availability—traffic must continue if one backend fails\n- Want to add/remove backends without client awareness\n- TLS termination or path-based routing required (L7 only)\n\n## Tradeoffs\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Round robin | Simple, stateless | Ignores load; long requests can skew distribution |\n| Least connections | Adapts to request duration | Requires connection tracking |\n| IP hash | Sticky without app support | Poor distribution if clients cluster; hot spots |\n| L4 | Fast, low overhead | No HTTP awareness, limited routing |\n| L7 | Path routing, header inspection, TLS | Higher latency, more CPU |\n\n## Failure scenarios\n\n- LB itself fails → single point of failure. Use multiple LBs behind DNS or anycast.\n- Health check too aggressive → flapping, backends removed unnecessarily.\n- Health check too slow → traffic to dead backend until detected.\n- Sticky sessions → backend crash loses session; failover complexity.\n\n## Scaling considerations",
    "tokens": [
      "load",
      "balancer",
      "what",
      "it",
      "is",
      "a",
      "component",
      "that",
      "distributes",
      "incoming",
      "traffic",
      "across",
      "multiple",
      "backend",
      "servers",
      "sits",
      "between",
      "clients",
      "and",
      "application",
      "servers",
      "can",
      "operate",
      "at",
      "l4",
      "tcp",
      "udp",
      "or",
      "l7",
      "http",
      "https",
      "when",
      "to",
      "use",
      "it",
      "-",
      "multiple",
      "app",
      "server",
      "instances",
      "for",
      "horizontal",
      "scaling",
      "-",
      "need",
      "high",
      "availability",
      "traffic",
      "must",
      "continue",
      "if",
      "one",
      "backend",
      "fails",
      "-",
      "want",
      "to",
      "add",
      "remove",
      "backends",
      "without",
      "client",
      "awareness",
      "-",
      "tls",
      "termination",
      "or",
      "path-based",
      "routing",
      "required",
      "l7",
      "only",
      "tradeoffs",
      "approach",
      "pros",
      "cons",
      "----------",
      "------",
      "------",
      "round",
      "robin",
      "simple",
      "stateless",
      "ignores",
      "load",
      "long",
      "requests",
      "can",
      "skew",
      "distribution",
      "least",
      "connections",
      "adapts",
      "to",
      "request",
      "duration",
      "requires",
      "connection",
      "tracking",
      "ip",
      "hash",
      "sticky",
      "without",
      "app",
      "support",
      "poor",
      "distribution",
      "if",
      "clients",
      "cluster",
      "hot",
      "spots",
      "l4",
      "fast",
      "low",
      "overhead",
      "no",
      "http",
      "awareness",
      "limited",
      "routing",
      "l7",
      "path",
      "routing",
      "header",
      "inspection",
      "tls",
      "higher",
      "latency",
      "more",
      "cpu",
      "failure",
      "scenarios",
      "-",
      "lb",
      "itself",
      "fails",
      "single",
      "point",
      "of",
      "failure",
      "use",
      "multiple",
      "lbs",
      "behind",
      "dns",
      "or",
      "anycast",
      "-",
      "health",
      "check",
      "too",
      "aggressive",
      "flapping",
      "backends",
      "removed",
      "unnecessarily",
      "-",
      "health",
      "check",
      "too",
      "slow",
      "traffic",
      "to",
      "dead",
      "backend",
      "until",
      "detected",
      "-",
      "sticky",
      "sessions",
      "backend",
      "crash",
      "loses",
      "session",
      "failover",
      "complexity",
      "scaling",
      "considerations"
    ]
  },
  {
    "id": "chunk-5",
    "docId": "load_balancer",
    "text": "- LB throughput limits (connections/sec, bandwidth). Scale vertically or add LBs.\n- Session state: avoid if possible; use external store if needed.\n- Backend capacity: add instances; LB auto-discovers via health checks.\n- Geographic: global LB (DNS, anycast) routes to nearest region.\n\n## Metrics to monitor\n\n- Request rate, error rate, latency p50/p99 per backend pool\n- Active connections per backend\n- Health check pass/fail rate\n- LB CPU, memory, connection count\n- Backend count, in/out of rotation\n\n## Real-world examples\n\n- NGINX, HAProxy: software LBs, L4/L7\n- AWS ALB/NLB: managed, auto-scaling\n- Cloudflare: global anycast, DDoS mitigation\n- Sticky sessions: e-commerce cart; stateful WebSocket\n",
    "tokens": [
      "-",
      "lb",
      "throughput",
      "limits",
      "connections",
      "sec",
      "bandwidth",
      "scale",
      "vertically",
      "or",
      "add",
      "lbs",
      "-",
      "session",
      "state",
      "avoid",
      "if",
      "possible",
      "use",
      "external",
      "store",
      "if",
      "needed",
      "-",
      "backend",
      "capacity",
      "add",
      "instances",
      "lb",
      "auto-discovers",
      "via",
      "health",
      "checks",
      "-",
      "geographic",
      "global",
      "lb",
      "dns",
      "anycast",
      "routes",
      "to",
      "nearest",
      "region",
      "metrics",
      "to",
      "monitor",
      "-",
      "request",
      "rate",
      "error",
      "rate",
      "latency",
      "p50",
      "p99",
      "per",
      "backend",
      "pool",
      "-",
      "active",
      "connections",
      "per",
      "backend",
      "-",
      "health",
      "check",
      "pass",
      "fail",
      "rate",
      "-",
      "lb",
      "cpu",
      "memory",
      "connection",
      "count",
      "-",
      "backend",
      "count",
      "in",
      "out",
      "of",
      "rotation",
      "real-world",
      "examples",
      "-",
      "nginx",
      "haproxy",
      "software",
      "lbs",
      "l4",
      "l7",
      "-",
      "aws",
      "alb",
      "nlb",
      "managed",
      "auto-scaling",
      "-",
      "cloudflare",
      "global",
      "anycast",
      "ddos",
      "mitigation",
      "-",
      "sticky",
      "sessions",
      "e-commerce",
      "cart",
      "stateful",
      "websocket"
    ]
  },
  {
    "id": "chunk-6",
    "docId": "caching",
    "text": "# Caching\n\n## What it is\n\nA layer that stores frequently accessed data in fast storage (memory, SSD) to reduce load on primary storage and lower latency. Sits between application and data source.\n\n## When to use it\n\n- Read-heavy workload; same data requested repeatedly\n- Primary store is slow or expensive for high QPS\n- Can tolerate stale data for some time (TTL)\n- Need sub-millisecond read latency for hot data\n\n## Tradeoffs\n\n| Pattern | Consistency | Latency | Complexity |\n|---------|-------------|---------|------------|\n| Cache-aside | App controls; stale possible | Low | Medium |\n| Read-through | Cache fills on miss | Low | Low |\n| Write-through | Strong; write to cache + DB | Higher write | Low |\n| Write-behind | Eventual; async flush | Lowest write | High; data loss risk |\n\n## Failure scenarios\n\n- Cache miss storm: cache cold or evicted; all traffic hits DB. Use warming, staggered TTL.\n- Stale reads: TTL too long or invalidation bug. Affects correctness.\n- Cache host fail: data lost if in-memory. Use replication or accept loss.\n- Thundering herd: one key expires; many requests hit DB. Use single-flight or probabilistic early expiry.\n\n## Scaling considerations",
    "tokens": [
      "caching",
      "what",
      "it",
      "is",
      "a",
      "layer",
      "that",
      "stores",
      "frequently",
      "accessed",
      "data",
      "in",
      "fast",
      "storage",
      "memory",
      "ssd",
      "to",
      "reduce",
      "load",
      "on",
      "primary",
      "storage",
      "and",
      "lower",
      "latency",
      "sits",
      "between",
      "application",
      "and",
      "data",
      "source",
      "when",
      "to",
      "use",
      "it",
      "-",
      "read-heavy",
      "workload",
      "same",
      "data",
      "requested",
      "repeatedly",
      "-",
      "primary",
      "store",
      "is",
      "slow",
      "or",
      "expensive",
      "for",
      "high",
      "qps",
      "-",
      "can",
      "tolerate",
      "stale",
      "data",
      "for",
      "some",
      "time",
      "ttl",
      "-",
      "need",
      "sub-millisecond",
      "read",
      "latency",
      "for",
      "hot",
      "data",
      "tradeoffs",
      "pattern",
      "consistency",
      "latency",
      "complexity",
      "---------",
      "-------------",
      "---------",
      "------------",
      "cache-aside",
      "app",
      "controls",
      "stale",
      "possible",
      "low",
      "medium",
      "read-through",
      "cache",
      "fills",
      "on",
      "miss",
      "low",
      "low",
      "write-through",
      "strong",
      "write",
      "to",
      "cache",
      "db",
      "higher",
      "write",
      "low",
      "write-behind",
      "eventual",
      "async",
      "flush",
      "lowest",
      "write",
      "high",
      "data",
      "loss",
      "risk",
      "failure",
      "scenarios",
      "-",
      "cache",
      "miss",
      "storm",
      "cache",
      "cold",
      "or",
      "evicted",
      "all",
      "traffic",
      "hits",
      "db",
      "use",
      "warming",
      "staggered",
      "ttl",
      "-",
      "stale",
      "reads",
      "ttl",
      "too",
      "long",
      "or",
      "invalidation",
      "bug",
      "affects",
      "correctness",
      "-",
      "cache",
      "host",
      "fail",
      "data",
      "lost",
      "if",
      "in-memory",
      "use",
      "replication",
      "or",
      "accept",
      "loss",
      "-",
      "thundering",
      "herd",
      "one",
      "key",
      "expires",
      "many",
      "requests",
      "hit",
      "db",
      "use",
      "single-flight",
      "or",
      "probabilistic",
      "early",
      "expiry",
      "scaling",
      "considerations"
    ]
  },
  {
    "id": "chunk-7",
    "docId": "caching",
    "text": "- Partition by key hash across shards. Add nodes; rebalance.\n- Memory bound per node. Scale horizontally.\n- Hit rate vs memory: larger cache = higher hit rate, diminishing returns.\n- Distributed cache (Redis Cluster, Memcached): network latency, consistency.\n\n## Metrics to monitor\n\n- Hit rate, miss rate\n- Latency p50/p99 for cache ops\n- Eviction rate, memory usage\n- Backend load (DB QPS) — should drop with cache\n- Stale read incidents (if measurable)\n\n## Real-world examples\n\n- CDN: static assets, images at edge\n- Redis/Memcached: session, API response cache\n- Browser cache: reduce server load\n- L1/L2 CPU cache: minimize main memory access\n",
    "tokens": [
      "-",
      "partition",
      "by",
      "key",
      "hash",
      "across",
      "shards",
      "add",
      "nodes",
      "rebalance",
      "-",
      "memory",
      "bound",
      "per",
      "node",
      "scale",
      "horizontally",
      "-",
      "hit",
      "rate",
      "vs",
      "memory",
      "larger",
      "cache",
      "higher",
      "hit",
      "rate",
      "diminishing",
      "returns",
      "-",
      "distributed",
      "cache",
      "redis",
      "cluster",
      "memcached",
      "network",
      "latency",
      "consistency",
      "metrics",
      "to",
      "monitor",
      "-",
      "hit",
      "rate",
      "miss",
      "rate",
      "-",
      "latency",
      "p50",
      "p99",
      "for",
      "cache",
      "ops",
      "-",
      "eviction",
      "rate",
      "memory",
      "usage",
      "-",
      "backend",
      "load",
      "db",
      "qps",
      "should",
      "drop",
      "with",
      "cache",
      "-",
      "stale",
      "read",
      "incidents",
      "if",
      "measurable",
      "real-world",
      "examples",
      "-",
      "cdn",
      "static",
      "assets",
      "images",
      "at",
      "edge",
      "-",
      "redis",
      "memcached",
      "session",
      "api",
      "response",
      "cache",
      "-",
      "browser",
      "cache",
      "reduce",
      "server",
      "load",
      "-",
      "l1",
      "l2",
      "cpu",
      "cache",
      "minimize",
      "main",
      "memory",
      "access"
    ]
  },
  {
    "id": "chunk-8",
    "docId": "sharding",
    "text": "# Sharding\n\n## What it is\n\nSplitting data across multiple independent database instances (shards) by a partition key. Each shard holds a subset of data. Enables horizontal scaling beyond single-node limits.\n\n## When to use it\n\n- Data size or write throughput exceeds single-node capacity\n- Need to scale writes; read replicas don't help write bottleneck\n- Acceptable to lose cross-shard transactions and complex joins\n- Data naturally partitionable (e.g. by user_id, tenant_id)\n\n## Tradeoffs\n\n| Strategy | Distribution | Range queries | Hot spots |\n|----------|--------------|---------------|-----------|\n| Hash | Even | Hard; scatter-gather | Rare |\n| Range | Skew possible | Easy | Boundary hot spots |\n| Directory | Flexible | Depends | Manual balance |\n\n## Failure scenarios\n\n- Hot partition: one shard gets disproportionate load. Mitigate: compound partition key, split hot shard.\n- Rebalancing: moving data causes load; can impact availability. Do gradually.\n- Cross-shard query: expensive or impossible. Design schema to avoid.\n- Shard failure: that partition unavailable. Replicate shards.\n\n## Scaling considerations\n\n- Add shards; rehash and migrate data. Downtime or online migration.\n- Partition key choice is critical; hard to change later.\n- Avoid shard that grows unbounded (e.g. global queue). Use sub-partitioning.\n- Consistent hashing reduces rebalancing churn.\n\n## Metrics to monitor",
    "tokens": [
      "sharding",
      "what",
      "it",
      "is",
      "splitting",
      "data",
      "across",
      "multiple",
      "independent",
      "database",
      "instances",
      "shards",
      "by",
      "a",
      "partition",
      "key",
      "each",
      "shard",
      "holds",
      "a",
      "subset",
      "of",
      "data",
      "enables",
      "horizontal",
      "scaling",
      "beyond",
      "single-node",
      "limits",
      "when",
      "to",
      "use",
      "it",
      "-",
      "data",
      "size",
      "or",
      "write",
      "throughput",
      "exceeds",
      "single-node",
      "capacity",
      "-",
      "need",
      "to",
      "scale",
      "writes",
      "read",
      "replicas",
      "don",
      "t",
      "help",
      "write",
      "bottleneck",
      "-",
      "acceptable",
      "to",
      "lose",
      "cross-shard",
      "transactions",
      "and",
      "complex",
      "joins",
      "-",
      "data",
      "naturally",
      "partitionable",
      "e",
      "g",
      "by",
      "user_id",
      "tenant_id",
      "tradeoffs",
      "strategy",
      "distribution",
      "range",
      "queries",
      "hot",
      "spots",
      "----------",
      "--------------",
      "---------------",
      "-----------",
      "hash",
      "even",
      "hard",
      "scatter-gather",
      "rare",
      "range",
      "skew",
      "possible",
      "easy",
      "boundary",
      "hot",
      "spots",
      "directory",
      "flexible",
      "depends",
      "manual",
      "balance",
      "failure",
      "scenarios",
      "-",
      "hot",
      "partition",
      "one",
      "shard",
      "gets",
      "disproportionate",
      "load",
      "mitigate",
      "compound",
      "partition",
      "key",
      "split",
      "hot",
      "shard",
      "-",
      "rebalancing",
      "moving",
      "data",
      "causes",
      "load",
      "can",
      "impact",
      "availability",
      "do",
      "gradually",
      "-",
      "cross-shard",
      "query",
      "expensive",
      "or",
      "impossible",
      "design",
      "schema",
      "to",
      "avoid",
      "-",
      "shard",
      "failure",
      "that",
      "partition",
      "unavailable",
      "replicate",
      "shards",
      "scaling",
      "considerations",
      "-",
      "add",
      "shards",
      "rehash",
      "and",
      "migrate",
      "data",
      "downtime",
      "or",
      "online",
      "migration",
      "-",
      "partition",
      "key",
      "choice",
      "is",
      "critical",
      "hard",
      "to",
      "change",
      "later",
      "-",
      "avoid",
      "shard",
      "that",
      "grows",
      "unbounded",
      "e",
      "g",
      "global",
      "queue",
      "use",
      "sub-partitioning",
      "-",
      "consistent",
      "hashing",
      "reduces",
      "rebalancing",
      "churn",
      "metrics",
      "to",
      "monitor"
    ]
  },
  {
    "id": "chunk-9",
    "docId": "sharding",
    "text": "- QPS, latency per shard\n- Data size, disk usage per shard\n- Rebalancing progress, migration lag\n- Hot partition detection (QPS skew)\n- Cross-shard query count (should be minimal)\n\n## Real-world examples\n\n- User data: shard by user_id; each shard has subset of users\n- Multi-tenant SaaS: shard by tenant_id\n- Time-series: range shard by time; old shards archived\n- Kafka: partition by key; each partition = shard\n",
    "tokens": [
      "-",
      "qps",
      "latency",
      "per",
      "shard",
      "-",
      "data",
      "size",
      "disk",
      "usage",
      "per",
      "shard",
      "-",
      "rebalancing",
      "progress",
      "migration",
      "lag",
      "-",
      "hot",
      "partition",
      "detection",
      "qps",
      "skew",
      "-",
      "cross-shard",
      "query",
      "count",
      "should",
      "be",
      "minimal",
      "real-world",
      "examples",
      "-",
      "user",
      "data",
      "shard",
      "by",
      "user_id",
      "each",
      "shard",
      "has",
      "subset",
      "of",
      "users",
      "-",
      "multi-tenant",
      "saas",
      "shard",
      "by",
      "tenant_id",
      "-",
      "time-series",
      "range",
      "shard",
      "by",
      "time",
      "old",
      "shards",
      "archived",
      "-",
      "kafka",
      "partition",
      "by",
      "key",
      "each",
      "partition",
      "shard"
    ]
  },
  {
    "id": "chunk-10",
    "docId": "databases",
    "text": "# Databases\n\n## What it is\n\nPersistent storage for structured (relational) or semi-structured (document, key-value) data. Provides ACID or relaxed guarantees. Primary-replica, sharding, indexing for scale.\n\n## When to use it\n\n- Need durable persistence; survive restarts\n- Need transactions, consistency guarantees\n- Need query capability (SQL, indexes, aggregation)\n- Strong consistency required for critical data\n\n## Tradeoffs\n\n| Type | Consistency | Scale | Use case |\n|------|-------------|-------|----------|\n| SQL | Strong, ACID | Vertical + replicas | Transactions, complex queries |\n| NoSQL doc | Tunable | Horizontal | Flexible schema, document access |\n| KV | Per-key | Horizontal | Simple lookup |\n| Wide-column | Tunable | Horizontal | Time-series, analytics |\n\n## Failure scenarios\n\n- Primary failure: promote replica; RPO/RTO depend on replication mode (sync/async).\n- Replication lag: reads from replica may be stale. Route read-after-write to primary.\n- Disk full, OOM: node down. Monitoring, alerts, capacity planning.\n- Split brain: two primaries if failover not coordinated. Use consensus (Paxos, Raft).\n\n## Scaling considerations\n\n- Read scaling: add replicas; route reads to replicas.\n- Write scaling: shard; each shard has own primary.\n- Connection pooling: limit connections per app instance.\n- Indexes: speed reads; slow writes. Right-size.\n\n## Metrics to monitor",
    "tokens": [
      "databases",
      "what",
      "it",
      "is",
      "persistent",
      "storage",
      "for",
      "structured",
      "relational",
      "or",
      "semi-structured",
      "document",
      "key-value",
      "data",
      "provides",
      "acid",
      "or",
      "relaxed",
      "guarantees",
      "primary-replica",
      "sharding",
      "indexing",
      "for",
      "scale",
      "when",
      "to",
      "use",
      "it",
      "-",
      "need",
      "durable",
      "persistence",
      "survive",
      "restarts",
      "-",
      "need",
      "transactions",
      "consistency",
      "guarantees",
      "-",
      "need",
      "query",
      "capability",
      "sql",
      "indexes",
      "aggregation",
      "-",
      "strong",
      "consistency",
      "required",
      "for",
      "critical",
      "data",
      "tradeoffs",
      "type",
      "consistency",
      "scale",
      "use",
      "case",
      "------",
      "-------------",
      "-------",
      "----------",
      "sql",
      "strong",
      "acid",
      "vertical",
      "replicas",
      "transactions",
      "complex",
      "queries",
      "nosql",
      "doc",
      "tunable",
      "horizontal",
      "flexible",
      "schema",
      "document",
      "access",
      "kv",
      "per-key",
      "horizontal",
      "simple",
      "lookup",
      "wide-column",
      "tunable",
      "horizontal",
      "time-series",
      "analytics",
      "failure",
      "scenarios",
      "-",
      "primary",
      "failure",
      "promote",
      "replica",
      "rpo",
      "rto",
      "depend",
      "on",
      "replication",
      "mode",
      "sync",
      "async",
      "-",
      "replication",
      "lag",
      "reads",
      "from",
      "replica",
      "may",
      "be",
      "stale",
      "route",
      "read-after-write",
      "to",
      "primary",
      "-",
      "disk",
      "full",
      "oom",
      "node",
      "down",
      "monitoring",
      "alerts",
      "capacity",
      "planning",
      "-",
      "split",
      "brain",
      "two",
      "primaries",
      "if",
      "failover",
      "not",
      "coordinated",
      "use",
      "consensus",
      "paxos",
      "raft",
      "scaling",
      "considerations",
      "-",
      "read",
      "scaling",
      "add",
      "replicas",
      "route",
      "reads",
      "to",
      "replicas",
      "-",
      "write",
      "scaling",
      "shard",
      "each",
      "shard",
      "has",
      "own",
      "primary",
      "-",
      "connection",
      "pooling",
      "limit",
      "connections",
      "per",
      "app",
      "instance",
      "-",
      "indexes",
      "speed",
      "reads",
      "slow",
      "writes",
      "right-size",
      "metrics",
      "to",
      "monitor"
    ]
  },
  {
    "id": "chunk-11",
    "docId": "databases",
    "text": "- QPS (read/write), latency p50/p99\n- Replication lag (seconds behind primary)\n- Connection count, pool utilization\n- Disk I/O, CPU, memory\n- Lock wait, deadlocks\n- Error rate, timeouts\n\n## Real-world examples\n\n- PostgreSQL, MySQL: relational, ACID\n- MongoDB: document store\n- Cassandra: wide-column, multi-DC\n- DynamoDB: managed KV, auto-scaling\n- Primary-replica: write to primary, read from replicas\n",
    "tokens": [
      "-",
      "qps",
      "read",
      "write",
      "latency",
      "p50",
      "p99",
      "-",
      "replication",
      "lag",
      "seconds",
      "behind",
      "primary",
      "-",
      "connection",
      "count",
      "pool",
      "utilization",
      "-",
      "disk",
      "i",
      "o",
      "cpu",
      "memory",
      "-",
      "lock",
      "wait",
      "deadlocks",
      "-",
      "error",
      "rate",
      "timeouts",
      "real-world",
      "examples",
      "-",
      "postgresql",
      "mysql",
      "relational",
      "acid",
      "-",
      "mongodb",
      "document",
      "store",
      "-",
      "cassandra",
      "wide-column",
      "multi-dc",
      "-",
      "dynamodb",
      "managed",
      "kv",
      "auto-scaling",
      "-",
      "primary-replica",
      "write",
      "to",
      "primary",
      "read",
      "from",
      "replicas"
    ]
  },
  {
    "id": "chunk-12",
    "docId": "message_queue",
    "text": "# Message Queue\n\n## What it is\n\nAsynchronous buffer between producers and consumers. Producers publish messages; consumers process when ready. Decouples services, absorbs spikes, enables async workflows.\n\n## When to use it\n\n- Decouple services; producer doesn't wait for consumer\n- Absorb traffic spikes; queue buffers bursts\n- Async processing (email, notifications, background jobs)\n- Event-driven architecture; multiple consumers per event\n\n## Tradeoffs\n\n| Semantics | Duplicates | Complexity |\n|-----------|------------|------------|\n| At-least-once | Possible | Low |\n| At-most-once | No; may lose | Low |\n| Exactly-once | No | High; idempotency, dedup |\n\n## Failure scenarios\n\n- Consumer slow: queue grows. Backpressure: block/reject producers.\n- Consumer crash: unacked messages re-queued. At-least-once = duplicates.\n- Queue full: producers blocked or reject. Need capacity planning.\n- Message poison: bad message crashes consumer repeatedly. DLQ.\n- Ordering: partition by key; global order expensive.\n\n## Scaling considerations\n\n- Partition by key for parallelism; order within partition.\n- Consumer groups: each partition consumed by one consumer in group.\n- Add partitions for more parallelism; rebalancing overhead.\n- Retention: longer = more storage; shorter = replay window.\n\n## Metrics to monitor\n\n- Queue depth, lag (messages behind)\n- Publish rate, consume rate\n- Latency: produce ack, end-to-end\n- DLQ depth, dead-letter rate\n- Consumer lag per partition\n- Error rate, retry count\n\n## Real-world examples",
    "tokens": [
      "message",
      "queue",
      "what",
      "it",
      "is",
      "asynchronous",
      "buffer",
      "between",
      "producers",
      "and",
      "consumers",
      "producers",
      "publish",
      "messages",
      "consumers",
      "process",
      "when",
      "ready",
      "decouples",
      "services",
      "absorbs",
      "spikes",
      "enables",
      "async",
      "workflows",
      "when",
      "to",
      "use",
      "it",
      "-",
      "decouple",
      "services",
      "producer",
      "doesn",
      "t",
      "wait",
      "for",
      "consumer",
      "-",
      "absorb",
      "traffic",
      "spikes",
      "queue",
      "buffers",
      "bursts",
      "-",
      "async",
      "processing",
      "email",
      "notifications",
      "background",
      "jobs",
      "-",
      "event-driven",
      "architecture",
      "multiple",
      "consumers",
      "per",
      "event",
      "tradeoffs",
      "semantics",
      "duplicates",
      "complexity",
      "-----------",
      "------------",
      "------------",
      "at-least-once",
      "possible",
      "low",
      "at-most-once",
      "no",
      "may",
      "lose",
      "low",
      "exactly-once",
      "no",
      "high",
      "idempotency",
      "dedup",
      "failure",
      "scenarios",
      "-",
      "consumer",
      "slow",
      "queue",
      "grows",
      "backpressure",
      "block",
      "reject",
      "producers",
      "-",
      "consumer",
      "crash",
      "unacked",
      "messages",
      "re-queued",
      "at-least-once",
      "duplicates",
      "-",
      "queue",
      "full",
      "producers",
      "blocked",
      "or",
      "reject",
      "need",
      "capacity",
      "planning",
      "-",
      "message",
      "poison",
      "bad",
      "message",
      "crashes",
      "consumer",
      "repeatedly",
      "dlq",
      "-",
      "ordering",
      "partition",
      "by",
      "key",
      "global",
      "order",
      "expensive",
      "scaling",
      "considerations",
      "-",
      "partition",
      "by",
      "key",
      "for",
      "parallelism",
      "order",
      "within",
      "partition",
      "-",
      "consumer",
      "groups",
      "each",
      "partition",
      "consumed",
      "by",
      "one",
      "consumer",
      "in",
      "group",
      "-",
      "add",
      "partitions",
      "for",
      "more",
      "parallelism",
      "rebalancing",
      "overhead",
      "-",
      "retention",
      "longer",
      "more",
      "storage",
      "shorter",
      "replay",
      "window",
      "metrics",
      "to",
      "monitor",
      "-",
      "queue",
      "depth",
      "lag",
      "messages",
      "behind",
      "-",
      "publish",
      "rate",
      "consume",
      "rate",
      "-",
      "latency",
      "produce",
      "ack",
      "end-to-end",
      "-",
      "dlq",
      "depth",
      "dead-letter",
      "rate",
      "-",
      "consumer",
      "lag",
      "per",
      "partition",
      "-",
      "error",
      "rate",
      "retry",
      "count",
      "real-world",
      "examples"
    ]
  },
  {
    "id": "chunk-13",
    "docId": "message_queue",
    "text": "- Kafka: event log, high throughput\n- RabbitMQ: flexible routing, various protocols\n- SQS: managed, at-least-once\n- Order processing: queue orders; inventory service consumes\n- Notifications: fan-out to email, push, SMS\n",
    "tokens": [
      "-",
      "kafka",
      "event",
      "log",
      "high",
      "throughput",
      "-",
      "rabbitmq",
      "flexible",
      "routing",
      "various",
      "protocols",
      "-",
      "sqs",
      "managed",
      "at-least-once",
      "-",
      "order",
      "processing",
      "queue",
      "orders",
      "inventory",
      "service",
      "consumes",
      "-",
      "notifications",
      "fan-out",
      "to",
      "email",
      "push",
      "sms"
    ]
  },
  {
    "id": "chunk-14",
    "docId": "consistency",
    "text": "# Consistency\n\n## What it is\n\nGuarantees about when a read sees a prior write. Strong: immediate. Eventual: after some time. Tune per use case; consistency trades with availability and latency.\n\n## When to use it\n\n- Strong: financial, inventory, critical state\n- Eventual: counters, analytics, non-critical UI\n- Read-your-writes: user sees own updates; route to same replica or primary\n\n## Tradeoffs\n\n| Model | Availability | Latency | Use case |\n|-------|--------------|---------|----------|\n| Strong | Lower | Higher | Critical |\n| Eventual | Higher | Lower | Non-critical |\n| Causal | Middle | Middle | Social, feeds |\n| Read-your-writes | Middle | Middle | User session |\n\n## Failure scenarios\n\n- Replication lag: read from replica sees stale data. Route read-after-write to primary.\n- Partition: CAP—choose consistency or availability. Often tune per operation.\n- Clock skew: version vectors or logical clocks for ordering without wall time.\n\n## Scaling considerations\n\n- Strong consistency: limited to primary or quorum; higher latency.\n- Eventual: read from any replica; scale reads; accept staleness.\n- Quorum: W + R > N; tune W, R for latency vs consistency.\n- Cross-region: eventual typically; strong requires sync replication.\n\n## Metrics to monitor\n\n- Replication lag\n- Stale read rate (if measurable)\n- Conflict rate (multi-writer)\n- Consistency-related bugs (user reports)\n\n## Real-world examples",
    "tokens": [
      "consistency",
      "what",
      "it",
      "is",
      "guarantees",
      "about",
      "when",
      "a",
      "read",
      "sees",
      "a",
      "prior",
      "write",
      "strong",
      "immediate",
      "eventual",
      "after",
      "some",
      "time",
      "tune",
      "per",
      "use",
      "case",
      "consistency",
      "trades",
      "with",
      "availability",
      "and",
      "latency",
      "when",
      "to",
      "use",
      "it",
      "-",
      "strong",
      "financial",
      "inventory",
      "critical",
      "state",
      "-",
      "eventual",
      "counters",
      "analytics",
      "non-critical",
      "ui",
      "-",
      "read-your-writes",
      "user",
      "sees",
      "own",
      "updates",
      "route",
      "to",
      "same",
      "replica",
      "or",
      "primary",
      "tradeoffs",
      "model",
      "availability",
      "latency",
      "use",
      "case",
      "-------",
      "--------------",
      "---------",
      "----------",
      "strong",
      "lower",
      "higher",
      "critical",
      "eventual",
      "higher",
      "lower",
      "non-critical",
      "causal",
      "middle",
      "middle",
      "social",
      "feeds",
      "read-your-writes",
      "middle",
      "middle",
      "user",
      "session",
      "failure",
      "scenarios",
      "-",
      "replication",
      "lag",
      "read",
      "from",
      "replica",
      "sees",
      "stale",
      "data",
      "route",
      "read-after-write",
      "to",
      "primary",
      "-",
      "partition",
      "cap",
      "choose",
      "consistency",
      "or",
      "availability",
      "often",
      "tune",
      "per",
      "operation",
      "-",
      "clock",
      "skew",
      "version",
      "vectors",
      "or",
      "logical",
      "clocks",
      "for",
      "ordering",
      "without",
      "wall",
      "time",
      "scaling",
      "considerations",
      "-",
      "strong",
      "consistency",
      "limited",
      "to",
      "primary",
      "or",
      "quorum",
      "higher",
      "latency",
      "-",
      "eventual",
      "read",
      "from",
      "any",
      "replica",
      "scale",
      "reads",
      "accept",
      "staleness",
      "-",
      "quorum",
      "w",
      "r",
      "n",
      "tune",
      "w",
      "r",
      "for",
      "latency",
      "vs",
      "consistency",
      "-",
      "cross-region",
      "eventual",
      "typically",
      "strong",
      "requires",
      "sync",
      "replication",
      "metrics",
      "to",
      "monitor",
      "-",
      "replication",
      "lag",
      "-",
      "stale",
      "read",
      "rate",
      "if",
      "measurable",
      "-",
      "conflict",
      "rate",
      "multi-writer",
      "-",
      "consistency-related",
      "bugs",
      "user",
      "reports",
      "real-world",
      "examples"
    ]
  },
  {
    "id": "chunk-15",
    "docId": "consistency",
    "text": "- Bank balance: strong consistency\n- Like count: eventual\n- DynamoDB: tunable per read/write\n- Cassandra: tunable consistency level\n- Quorum: 3 replicas; W=2, R=2\n",
    "tokens": [
      "-",
      "bank",
      "balance",
      "strong",
      "consistency",
      "-",
      "like",
      "count",
      "eventual",
      "-",
      "dynamodb",
      "tunable",
      "per",
      "read",
      "write",
      "-",
      "cassandra",
      "tunable",
      "consistency",
      "level",
      "-",
      "quorum",
      "3",
      "replicas",
      "w",
      "2",
      "r",
      "2"
    ]
  },
  {
    "id": "chunk-16",
    "docId": "capacity_estimation",
    "text": "# Capacity Estimation\n\n## What it is\n\nBack-of-envelope math to size infrastructure: QPS, storage, bandwidth. Done early in design to identify bottlenecks and ballpark costs.\n\n## When to use it\n\n- System design interview; show quantitative thinking\n- Pre-launch capacity planning\n- Cost estimation\n- Identifying scaling limits\n\n## Tradeoffs\n\n- Rough vs precise: rough is fast; precise needs real traffic.\n- Assumptions matter: DAU, read/write ratio, payload size.\n- Growth: 2x, 5x, 10x headroom typical.\n\n## Failure scenarios\n\n- Under-provisioned: overload at launch. Scale up; add caching.\n- Over-provisioned: wasted cost. Right-size with monitoring.\n- Wrong assumptions: traffic pattern differs. Iterate.\n\n## Scaling considerations\n\n- Start with reads: usually 10:1 or 100:1 read:write.\n- Storage: retention × daily writes × replication factor.\n- Bandwidth: request size × QPS × 2 (in + out).\n- Peaks: 2–3x average common.\n\n## Metrics to monitor\n\n- Actual QPS vs estimated\n- Storage growth vs projection\n- Bandwidth utilization\n- Cost vs budget\n\n## Real-world examples\n\n- 1M DAU, 10 requests/user/day → ~120 QPS average; 300–400 peak\n- 1KB average request → 400 KB/s = 3.2 Mbps\n- 100M users, 1KB profile, 3 replicas → 300 TB storage\n- Order of magnitude; refine with real data\n",
    "tokens": [
      "capacity",
      "estimation",
      "what",
      "it",
      "is",
      "back-of-envelope",
      "math",
      "to",
      "size",
      "infrastructure",
      "qps",
      "storage",
      "bandwidth",
      "done",
      "early",
      "in",
      "design",
      "to",
      "identify",
      "bottlenecks",
      "and",
      "ballpark",
      "costs",
      "when",
      "to",
      "use",
      "it",
      "-",
      "system",
      "design",
      "interview",
      "show",
      "quantitative",
      "thinking",
      "-",
      "pre-launch",
      "capacity",
      "planning",
      "-",
      "cost",
      "estimation",
      "-",
      "identifying",
      "scaling",
      "limits",
      "tradeoffs",
      "-",
      "rough",
      "vs",
      "precise",
      "rough",
      "is",
      "fast",
      "precise",
      "needs",
      "real",
      "traffic",
      "-",
      "assumptions",
      "matter",
      "dau",
      "read",
      "write",
      "ratio",
      "payload",
      "size",
      "-",
      "growth",
      "2x",
      "5x",
      "10x",
      "headroom",
      "typical",
      "failure",
      "scenarios",
      "-",
      "under-provisioned",
      "overload",
      "at",
      "launch",
      "scale",
      "up",
      "add",
      "caching",
      "-",
      "over-provisioned",
      "wasted",
      "cost",
      "right-size",
      "with",
      "monitoring",
      "-",
      "wrong",
      "assumptions",
      "traffic",
      "pattern",
      "differs",
      "iterate",
      "scaling",
      "considerations",
      "-",
      "start",
      "with",
      "reads",
      "usually",
      "10",
      "1",
      "or",
      "100",
      "1",
      "read",
      "write",
      "-",
      "storage",
      "retention",
      "daily",
      "writes",
      "replication",
      "factor",
      "-",
      "bandwidth",
      "request",
      "size",
      "qps",
      "2",
      "in",
      "out",
      "-",
      "peaks",
      "2",
      "3x",
      "average",
      "common",
      "metrics",
      "to",
      "monitor",
      "-",
      "actual",
      "qps",
      "vs",
      "estimated",
      "-",
      "storage",
      "growth",
      "vs",
      "projection",
      "-",
      "bandwidth",
      "utilization",
      "-",
      "cost",
      "vs",
      "budget",
      "real-world",
      "examples",
      "-",
      "1m",
      "dau",
      "10",
      "requests",
      "user",
      "day",
      "120",
      "qps",
      "average",
      "300",
      "400",
      "peak",
      "-",
      "1kb",
      "average",
      "request",
      "400",
      "kb",
      "s",
      "3",
      "2",
      "mbps",
      "-",
      "100m",
      "users",
      "1kb",
      "profile",
      "3",
      "replicas",
      "300",
      "tb",
      "storage",
      "-",
      "order",
      "of",
      "magnitude",
      "refine",
      "with",
      "real",
      "data"
    ]
  },
  {
    "id": "chunk-17",
    "docId": "failure_modes",
    "text": "# Failure Modes\n\n## What it is\n\nWays a system can fail: hardware, network, software, overload. Design for detection, mitigation, and recovery. Assume failures will happen.\n\n## When to use it\n\n- Design phase: identify single points of failure\n- Operations: runbooks, playbooks\n- Post-incident: categorize and prevent recurrence\n\n## Tradeoffs\n\n- Redundancy vs cost: more replicas = higher cost\n- Fail fast vs retry: fail fast simplifies; retry masks transient failures\n- Circuit breaker: stops cascading; adds complexity\n\n## Failure scenarios\n\n- Node failure: replica takes over; RTO depends on detection + failover.\n- Network partition: split brain risk; quorum, fencing.\n- Disk full: no writes; monitoring, alerting.\n- Slow dependency: timeout, circuit breaker, fallback.\n- Thundering herd: many retries; exponential backoff, jitter.\n- Cascading: one failure triggers others; circuit breaker, bulkheads.\n\n## Scaling considerations\n\n- More nodes: more failure domains; smaller blast radius.\n- Cross-region: regional outage; failover to another region.\n- Chaos engineering: inject failures; validate resilience.\n\n## Metrics to monitor\n\n- Error rate, timeout rate\n- Latency degradation\n- Dependency health\n- Failover events, RTO actual\n- Circuit breaker trips\n\n## Real-world examples",
    "tokens": [
      "failure",
      "modes",
      "what",
      "it",
      "is",
      "ways",
      "a",
      "system",
      "can",
      "fail",
      "hardware",
      "network",
      "software",
      "overload",
      "design",
      "for",
      "detection",
      "mitigation",
      "and",
      "recovery",
      "assume",
      "failures",
      "will",
      "happen",
      "when",
      "to",
      "use",
      "it",
      "-",
      "design",
      "phase",
      "identify",
      "single",
      "points",
      "of",
      "failure",
      "-",
      "operations",
      "runbooks",
      "playbooks",
      "-",
      "post-incident",
      "categorize",
      "and",
      "prevent",
      "recurrence",
      "tradeoffs",
      "-",
      "redundancy",
      "vs",
      "cost",
      "more",
      "replicas",
      "higher",
      "cost",
      "-",
      "fail",
      "fast",
      "vs",
      "retry",
      "fail",
      "fast",
      "simplifies",
      "retry",
      "masks",
      "transient",
      "failures",
      "-",
      "circuit",
      "breaker",
      "stops",
      "cascading",
      "adds",
      "complexity",
      "failure",
      "scenarios",
      "-",
      "node",
      "failure",
      "replica",
      "takes",
      "over",
      "rto",
      "depends",
      "on",
      "detection",
      "failover",
      "-",
      "network",
      "partition",
      "split",
      "brain",
      "risk",
      "quorum",
      "fencing",
      "-",
      "disk",
      "full",
      "no",
      "writes",
      "monitoring",
      "alerting",
      "-",
      "slow",
      "dependency",
      "timeout",
      "circuit",
      "breaker",
      "fallback",
      "-",
      "thundering",
      "herd",
      "many",
      "retries",
      "exponential",
      "backoff",
      "jitter",
      "-",
      "cascading",
      "one",
      "failure",
      "triggers",
      "others",
      "circuit",
      "breaker",
      "bulkheads",
      "scaling",
      "considerations",
      "-",
      "more",
      "nodes",
      "more",
      "failure",
      "domains",
      "smaller",
      "blast",
      "radius",
      "-",
      "cross-region",
      "regional",
      "outage",
      "failover",
      "to",
      "another",
      "region",
      "-",
      "chaos",
      "engineering",
      "inject",
      "failures",
      "validate",
      "resilience",
      "metrics",
      "to",
      "monitor",
      "-",
      "error",
      "rate",
      "timeout",
      "rate",
      "-",
      "latency",
      "degradation",
      "-",
      "dependency",
      "health",
      "-",
      "failover",
      "events",
      "rto",
      "actual",
      "-",
      "circuit",
      "breaker",
      "trips",
      "real-world",
      "examples"
    ]
  },
  {
    "id": "chunk-18",
    "docId": "failure_modes",
    "text": "- Netflix Chaos Monkey: random instance kill\n- Circuit breaker: stop calling failing service\n- Bulkhead: isolate thread pool per dependency\n- Timeout: 99% under 100ms; fail at 200ms\n- Retry with exponential backoff: 1s, 2s, 4s\n",
    "tokens": [
      "-",
      "netflix",
      "chaos",
      "monkey",
      "random",
      "instance",
      "kill",
      "-",
      "circuit",
      "breaker",
      "stop",
      "calling",
      "failing",
      "service",
      "-",
      "bulkhead",
      "isolate",
      "thread",
      "pool",
      "per",
      "dependency",
      "-",
      "timeout",
      "99",
      "under",
      "100ms",
      "fail",
      "at",
      "200ms",
      "-",
      "retry",
      "with",
      "exponential",
      "backoff",
      "1s",
      "2s",
      "4s"
    ]
  },
  {
    "id": "chunk-19",
    "docId": "interview_patterns",
    "text": "# Interview Patterns\n\n## What it is\n\nCommon structure and topics in system design interviews. Interviewers expect clarity on requirements, high-level design, deep dives, tradeoffs, and scaling.\n\n## When to use it\n\n- Preparing for system design interviews\n- Evaluating candidate responses\n- Structuring design discussions\n\n## Tradeoffs\n\n- Breadth vs depth: cover more vs go deeper on fewer\n- Whiteboard vs conversational: visual helps; talking shows reasoning\n- Time box: 45 min typical; prioritize critical path\n\n## Failure scenarios\n\n- Vague requirements: design wrong system. Ask clarifying questions.\n- Jump to solution: miss key constraints. Requirements first.\n- No numbers: \"it scales\" unconvincing. Capacity estimation.\n- Ignore failure: \"assume it doesn't fail\" weak. Discuss failure modes.\n\n## Scaling considerations\n\n- Start simple: single box, then add components\n- Explain why each component: load balancer for multiple app servers, etc.\n- End with tradeoffs: what you sacrificed and why\n\n## Metrics to monitor\n\n- Time spent per phase\n- Depth of follow-up answers\n- Coverage of: requirements, HLD, APIs, data model, scaling, failure\n\n## Real-world examples",
    "tokens": [
      "interview",
      "patterns",
      "what",
      "it",
      "is",
      "common",
      "structure",
      "and",
      "topics",
      "in",
      "system",
      "design",
      "interviews",
      "interviewers",
      "expect",
      "clarity",
      "on",
      "requirements",
      "high-level",
      "design",
      "deep",
      "dives",
      "tradeoffs",
      "and",
      "scaling",
      "when",
      "to",
      "use",
      "it",
      "-",
      "preparing",
      "for",
      "system",
      "design",
      "interviews",
      "-",
      "evaluating",
      "candidate",
      "responses",
      "-",
      "structuring",
      "design",
      "discussions",
      "tradeoffs",
      "-",
      "breadth",
      "vs",
      "depth",
      "cover",
      "more",
      "vs",
      "go",
      "deeper",
      "on",
      "fewer",
      "-",
      "whiteboard",
      "vs",
      "conversational",
      "visual",
      "helps",
      "talking",
      "shows",
      "reasoning",
      "-",
      "time",
      "box",
      "45",
      "min",
      "typical",
      "prioritize",
      "critical",
      "path",
      "failure",
      "scenarios",
      "-",
      "vague",
      "requirements",
      "design",
      "wrong",
      "system",
      "ask",
      "clarifying",
      "questions",
      "-",
      "jump",
      "to",
      "solution",
      "miss",
      "key",
      "constraints",
      "requirements",
      "first",
      "-",
      "no",
      "numbers",
      "it",
      "scales",
      "unconvincing",
      "capacity",
      "estimation",
      "-",
      "ignore",
      "failure",
      "assume",
      "it",
      "doesn",
      "t",
      "fail",
      "weak",
      "discuss",
      "failure",
      "modes",
      "scaling",
      "considerations",
      "-",
      "start",
      "simple",
      "single",
      "box",
      "then",
      "add",
      "components",
      "-",
      "explain",
      "why",
      "each",
      "component",
      "load",
      "balancer",
      "for",
      "multiple",
      "app",
      "servers",
      "etc",
      "-",
      "end",
      "with",
      "tradeoffs",
      "what",
      "you",
      "sacrificed",
      "and",
      "why",
      "metrics",
      "to",
      "monitor",
      "-",
      "time",
      "spent",
      "per",
      "phase",
      "-",
      "depth",
      "of",
      "follow-up",
      "answers",
      "-",
      "coverage",
      "of",
      "requirements",
      "hld",
      "apis",
      "data",
      "model",
      "scaling",
      "failure",
      "real-world",
      "examples"
    ]
  },
  {
    "id": "chunk-20",
    "docId": "interview_patterns",
    "text": "- Clarify: \"1M DAU or 1M QPS?\" \"Read-heavy or write-heavy?\"\n- High-level: \"Client → LB → App servers → DB. Add cache for reads.\"\n- Deep dive: \"For cache, what key? TTL? Invalidation?\"\n- Tradeoff: \"Strong consistency here; eventual for analytics.\"\n- Interview phases: requirements → HLD → APIs/data → scaling → failure → wrap-up\n",
    "tokens": [
      "-",
      "clarify",
      "1m",
      "dau",
      "or",
      "1m",
      "qps",
      "read-heavy",
      "or",
      "write-heavy",
      "-",
      "high-level",
      "client",
      "lb",
      "app",
      "servers",
      "db",
      "add",
      "cache",
      "for",
      "reads",
      "-",
      "deep",
      "dive",
      "for",
      "cache",
      "what",
      "key",
      "ttl",
      "invalidation",
      "-",
      "tradeoff",
      "strong",
      "consistency",
      "here",
      "eventual",
      "for",
      "analytics",
      "-",
      "interview",
      "phases",
      "requirements",
      "hld",
      "apis",
      "data",
      "scaling",
      "failure",
      "wrap-up"
    ]
  }
]